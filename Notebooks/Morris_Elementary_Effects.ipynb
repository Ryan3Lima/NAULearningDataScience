{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aeff845",
   "metadata": {},
   "source": [
    "# Morris Sensitivity Analysis (Elementary Effects Method)\n",
    "\n",
    "In global sensitivity analysis, the **Morris Method** (also called the *Method of Elementary Effects*) is a screening technique used to identify which input variables in a model have the greatest influence on the output. It was introduced by Max D. Morris in 1991 as a computationally efficient way to explore sensitivity in models with many inputs, without requiring an enormous number of model runs.\n",
    "\n",
    "The basic idea is:\n",
    "- Change one input at a time by a small step,\n",
    "- See how much the output changes,\n",
    "- Repeat this from different starting points across the input space,\n",
    "- Summarize how consistently (or inconsistently) each input causes change.\n",
    "\n",
    "That small one-at-a-time change in an input and the resulting change in the model output is called an **elementary effect**.\n",
    "\n",
    "---\n",
    "\n",
    "## Why was Morris developed?\n",
    "\n",
    "Before Morris (1991), sensitivity analysis often lived in two extremes:\n",
    "\n",
    "- **Local / derivative-based sensitivity**:  \n",
    "\n",
    "  This asks “If I make a tiny change to one parameter around the current baseline, how much does the output change?”  \n",
    "  This is basically a partial derivative.  \n",
    "  Problem: it only tells you about behavior *near one point* in parameter space, and it assumes smooth / linear behavior.\n",
    "\n",
    "- **Full global variance-based methods (like Sobol indices)**:  \n",
    "\n",
    "  These methods try to quantify how much of the total output variance is explained by each input and by their interactions.  \n",
    "  They’re extremely informative — but also computationally expensive, because they require a lot of model evaluations.\n",
    "\n",
    "Max Morris was looking for something in-between:\n",
    "\n",
    "- A method that is **global** (explores the whole parameter space, not just one point),\n",
    "- But still **cheap enough** to run early, even for high-dimensional problems (many inputs),\n",
    "- And able to flag inputs that are likely important, nonlinear, or interacting.\n",
    "\n",
    "So the Morris method is often described as a **screening method**: it’s a first pass that tells you which inputs matter and how they matter, so you know where to focus more detailed analysis later.\n",
    "\n",
    "---\n",
    "\n",
    "## What problems does the Morris method solve?\n",
    "\n",
    "Imagine you have a model:\n",
    "\n",
    "$$\n",
    "y = f(x_1, x_2, x_3, \\dots, x_k)\n",
    "$$\n",
    "\n",
    "where each $x_i$ is an input (a criterion weight, a threshold, a soil parameter, etc.), and $y$ is some decision score (e.g. total suitability, predicted recharge, contaminant load, habitat score).\n",
    "\n",
    "You want to know:\n",
    "\n",
    "1. Which inputs have basically **no effect** on $y$? (Those might be safely fixed or ignored.)\n",
    "2. Which inputs have a **large overall effect** on $y$? (These are important drivers.)\n",
    "3. Which inputs behave **nonlinearly** or **interact** with other inputs?  \n",
    "   (For example, “slope only matters once soil permeability is high,” or “forest cover and precipitation together change infiltration potential in a way you don’t get by looking at either alone.”)\n",
    "\n",
    "The Morris method gives you exactly that information with two summary statistics per input.\n",
    "\n",
    "---\n",
    "\n",
    "## Core concept: the Elementary Effect\n",
    "\n",
    "For each input $x_i$, we define an *elementary effect* as:\n",
    "\n",
    "$$\n",
    "EE_i = \\frac{f(x_1, \\dots, x_i + \\Delta, \\dots, x_k) - f(x_1, \\dots, x_i, \\dots, x_k)}{\\Delta}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\Delta$ is a small step in the value of $x_i$,\n",
    "- All other inputs are held constant for that step,\n",
    "- The numerator is “how much the output changed when we nudged just $x_i$.”\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- $EE_i$ is basically: “If I change only $x_i$ a little, how much does the model output respond?”\n",
    "\n",
    "But — and this is the key difference from local sensitivity — we don’t do this just once from one baseline. We repeat this from *multiple random locations* in the input space. Each repetition gives us another possible elementary effect for that same input.\n",
    "\n",
    "So for each input $x_i$, we don’t just get one number. We get a distribution of elementary effects across the space of plausible inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## Morris summary metrics\n",
    "\n",
    "After computing many elementary effects for each input, we summarize them. The two most common summaries are:\n",
    "\n",
    "1. **$\\mu^*$** (mu star):  \n",
    "\n",
    "   The mean of the *absolute value* of the elementary effects for that input.  \n",
    "   - High $\\mu^*$ means: changing this input tends to cause a big change in the output overall.  \n",
    "   - This is interpreted as “overall importance” or “influence strength.”\n",
    "\n",
    "   We use the absolute value so positive and negative effects don’t cancel each other out.\n",
    "\n",
    "2. **$\\sigma$** (sigma):  \n",
    "\n",
    "   The standard deviation of the elementary effects for that input.  \n",
    "   - High $\\sigma$ means: the effect of this input is not consistent — sometimes it has a big effect, sometimes small, sometimes positive, sometimes negative.  \n",
    "   - That usually indicates **nonlinearity** or **interactions with other inputs**.\n",
    "\n",
    "   Intuition: if an input only mattered under certain combinations of other inputs, you’d see a wide spread in its elementary effects → high σ.\n",
    "\n",
    "This gives you a beautiful diagnostic plot: μ\\* on the x-axis (importance) vs σ on the y-axis (interaction / nonlinearity).\n",
    "\n",
    "- Inputs with **low μ\\*** and **low σ** → mostly irrelevant.\n",
    "- Inputs with **high μ\\*** and **low σ** → consistently important, mostly linear effect on the output.\n",
    "- Inputs with **high μ\\*** and **high σ** → important but tricky: nonlinear or involved in interactions.\n",
    "\n",
    "That’s the classic “Morris scatter plot.”\n",
    "\n",
    "---\n",
    "\n",
    "## How it works (conceptually)\n",
    "\n",
    "1. Define ranges (or distributions) for each input $x_i$.  \n",
    "   Example: slope weight in WLC could vary from 0.1 to 0.4, precipitation weight from 0.2 to 0.6, etc.\n",
    "\n",
    "2. Sample a sequence of points in that input space (called “trajectories” or “paths”).  \n",
    "   Each path walks through the space one input at a time, changing one variable by $\\Delta$ while keeping the others fixed, then moving on to the next variable, etc.\n",
    "\n",
    "3. For each step along that path, compute the elementary effect $EE_i$.\n",
    "\n",
    "4. Aggregate all the elementary effects for each input across all paths → get $μ^*$ and $\\sigma$.\n",
    "\n",
    "5. Rank or plot inputs based on $\\mu^*$ and σ to decide which inputs matter.\n",
    "\n",
    "This is global because you’re sampling across the full allowable range of inputs — not just perturbing around a single baseline point.\n",
    "\n",
    "---\n",
    "\n",
    "## Why this is used\n",
    "\n",
    "The Morris method is widely used in:\n",
    "\n",
    "- Environmental modeling and hydrology (e.g., identifying which hydrogeologic parameters most influence recharge estimates or contaminant transport),\n",
    "- Ecological and habitat suitability modeling,\n",
    "- Groundwater recharge / infiltration models,\n",
    "- Flood and erosion models,\n",
    "- Multi-criteria decision analysis (MCDA), including GIS-based suitability mapping, to see which criteria weights dominate the final suitability score, and where there are strong interactions.\n",
    "\n",
    "In practice:\n",
    "\n",
    "- You run Morris first to screen out unimportant variables (so you don’t waste computation on them),\n",
    "- Then you apply heavier methods (like Sobol variance decomposition) on the variables that survived screening.\n",
    "\n",
    "So Morris is both:\n",
    "\n",
    "1. A science tool (which parameters actually matter?),\n",
    "2. A workflow tool (where should I spend my expensive computation time?).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The Morris method is a global, one-factor-at-a-time sensitivity screening method introduced by Max D. Morris in 1991.\n",
    "- It’s built around **elementary effects**: the change in model output when you nudge one input while holding others constant.\n",
    "- By repeating that across many starting points, you get a *distribution* of effects for each input.\n",
    "- You then summarize each input with:\n",
    "  - **$\\mu^*$** (mean absolute elementary effect): how influential this input is overall,\n",
    "  - **$\\sigma$** (stdev of elementary effects): how nonlinear or interaction-heavy its influence is.\n",
    "- This is incredibly helpful in decision-support models (like WLC suitability mapping) because it tells you:\n",
    "  - which criteria weights dominate suitability,\n",
    "  - which ones only matter in combination,\n",
    "  - and which ones are basically irrelevant.\n",
    "\n",
    "---\n",
    "\n",
    "Next, we’ll:\n",
    "\n",
    "1. Build a tiny synthetic model in Python so you can *see* elementary effects for a toy function,\n",
    "2. Compute $\\mu^*$ and $\\sigma$ for each input manually,\n",
    "3. Reproduce what SALib’s `morris` routines do,\n",
    "4. Visualize $\\mu^*$ vs $\\sigma$,\n",
    "5. Then connect that to a spatial WLC / MCDA setting.\n",
    "\n",
    "> Now let’s start generating elementary effects for a simple model in Python."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
