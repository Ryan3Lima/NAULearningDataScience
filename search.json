[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NAULearningDataScience",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "Notebooks/Kendalls_tau.html",
    "href": "Notebooks/Kendalls_tau.html",
    "title": "Kendall rank correlation coefficient (Kendall’s tau)",
    "section": "",
    "text": "The “probability” view\nIn statistics, the Kendall rank correlation coefficient, commonly referred to as Kendall’s tau (τ), is a statistic used to measure the ordinal association between two measured quantities. A T test is a non-parametric hypothesis test for statistical dependence based on the \\(T\\) coefficient. It is a measure of rank correlation: the similarity of the orderings of data when ranked by each of the quantities. It is named after Maurice Kendall, who developed it in 1938, though Gustav Fechner has proposed a similar measure in the context of time series in 1897.\nKendall’s motification was to create a rubust, intuitive measure of association between two rankings–one that was: - non parameteric – meaning it made no assumptions about the distribution of the data - intuative in interpretation – meaning it could be easily understood (based on concordant and discordant pairs) and - suitable for ordinal or ranked data (like preferences, ratings, or scores).\nBefore Kendall’s tau, other correlation measures like Pearson’s correlation coefficient were commonly used, but they assumed linear relationships and required interval or ratio data. Kendall’s tau provided a way to assess relationships in a more flexible manner, especially for non-linear or non-parametric data.\nIn many real-world problems, especially in decision analysis (also social sciences) data are often ordinal – things we can rank but not measure on a precise numerical scale.\nFor example: a hydrologist looking to build a groundwater recharge project, might want to rank potential sites based on suitability critiera, and rank them in suitability from 1 (low suitability) to 5 or 10 (high suitability). Kendall’s tau would allow the hydrologist to assess the association between different ranking criteria (like soil type, proximity to water sources, land use, etc.) without making assumptions about the underlying data distribution.\n## How it works\nImagine two people rank the same set of sites independently based on their expert opinion of suitability for groundwater recharge.\nKendall’s tau looks at all possible pairs of sites (S1 vs S2, S1 vs S3, … S4 vs S5) and asks: - Do the two analysts agree on which site should be ranked higher?\nFor any pair of sites \\((i, j)\\): - The pair is concordant if both analysts put the same site higher.\n(Example: if \\(A\\) says \\(S2\\) better than \\(S5\\), and \\(B\\) also says \\(S2\\) better than \\(S5\\).) - The pair is discordant if the analysts disagree about which one is better.\n(Example: A says \\(S1\\) better than \\(S4\\), but \\(B\\) says \\(S4\\) better than \\(S1\\).) - (Ties are possible in general, though not shown in this simple example. We handle those with slight variations of tau.)\nIntuition: - If most pairs are concordant → \\(\\tau\\) is close to +1 (the rankings mostly agree). - If most pairs are discordant → \\(\\tau\\) is close to −1 (the rankings mostly disagree / almost inverted). - If agreement and disagreement are about equal → \\(\\tau\\) is near 0.\nOne clean way to define Kendall’s tau is:\n\\[\n\\tau = P(\\text{concordant}) - P(\\text{discordant})\n\\]\nHere \\(P(\\text{concordant})\\) means:\n“Out of all possible pairs of items, what fraction of pairs are concordant?”\nIn other words, these are not probabilities in the sense of randomness over repeated experiments — they are proportions over all \\(\\frac{n(n-1)}{2}\\) pairs in this dataset.\nSo you can read \\(\\tau\\) as: &gt; “If I pick two items at random, how much more likely is it that the two rankings agree on their order than disagree?”\nThat’s the core interpretation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kendall rank correlation coefficient (Kendall's tau)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Kendalls_tau.html#the-counting-pairwise-formula",
    "href": "Notebooks/Kendalls_tau.html#the-counting-pairwise-formula",
    "title": "Kendall rank correlation coefficient (Kendall’s tau)",
    "section": "The counting (pairwise) formula",
    "text": "The counting (pairwise) formula\nLet: - \\(n\\) = number of items being ranked\n- \\(C\\) = number of concordant pairs\n- \\(D\\) = number of discordant pairs\n- \\(T = \\frac{n(n-1)}{2}\\) = total number of distinct pairs\nThen Kendall’s tau can be written as:\n\\[\n\\tau = \\frac{C - D}{T}\n= \\frac{C - D}{\\frac{1}{2} n (n - 1)}\n\\]\nThis is the same as the “probability” version, just written in terms of counts instead of proportions: - \\(\\frac{C}{T}\\) is \\(P(\\text{concordant})\\) - \\(\\frac{D}{T}\\) is \\(P(\\text{discordant})\\)\nSo: \\[\n\\tau = \\frac{C}{T} - \\frac{D}{T}\n\\]\n\nIn practice, we’ll compute \\(C\\) and \\(D\\) from two ranked lists, calculate \\(\\tau\\), and then visualize where disagreements are happening spatially or across alternatives.\nNext, we’ll implement this calculation in Python, both “by hand” (to see C and D) and using scipy.stats.kendalltau.\n\nNow lets explore how to calculate Kendall’s tau using Python.\n\n\n# import pandas to create an manipulate dataframes\nimport pandas as pd\n\n# create a sample dataframe with rankings from two analysts\ndata = pd.DataFrame({\n    \"Site\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n    \"Rank_Analyst1\": [1, 2, 3, 4, 5, 6],  # Analyst 1 thinks A&gt;B&gt;C&gt;D&gt;E&gt;F\n    \"Rank_Analyst2\": [1, 3, 2, 4, 6, 5]   # Analyst 2 mostly agrees, but swaps B/C and E/F\n})\n\ndata\n\n\n\n\n\n\n\n\nSite\nRank_Analyst1\nRank_Analyst2\n\n\n\n\n0\nA\n1\n1\n\n\n1\nB\n2\n3\n\n\n2\nC\n3\n2\n\n\n3\nD\n4\n4\n\n\n4\nE\n5\n6\n\n\n5\nF\n6\n5\n\n\n\n\n\n\n\n\nOk, now lets create a function that checks all possible pairings rankings to determine concordant (agreeing) and discordant (disagreeing) pairs.\n\n\n# itertools is a useful library for creating combinations and permutations\nimport itertools\n\ndef kendall_concordance_table(df, col_x, col_y):\n    \"\"\"\n    Create a table showing concordant and discordant pairs between two rankings.\n    df: DataFrame with rankings\n    col_x: column name for first ranking\n    col_y: column name for second ranking\n    Returns a DataFrame with pairwise comparisons and counts of concordant/discordant pairs.\n    \"\"\"\n    pairs_info = [] # to store info about each pair\n    C = 0  # concordant = they agree on order\n    D = 0  # discordant = they disagree on order\n    for (i, j) in itertools.combinations(df.index, 2): # for all unique pairs of indices (i, j)\n        x_i = df.loc[i, col_x] # x_i is the rank of item i in ranking x\n        x_j = df.loc[j, col_x] # x_j is the rank of item j in ranking x\n        y_i = df.loc[i, col_y] # y_i is the rank of item i in ranking y\n        y_j = df.loc[j, col_y] # y_j is the rank of item j in ranking y\n        site_i = df.loc[i, \"Site\"] # get site names for reporting for item i\n        site_j = df.loc[j, \"Site\"] # get site names for reporting for item j\n\n        # Compare pair ordering in each ranking\n        diff_x = x_i - x_j # difference in ranking for pair (i, j) in ranking x\n        diff_y = y_i - y_j # difference in ranking for pair (i, j) in ranking y\n\n        # If both differences have same sign -&gt; concordant\n        # If opposite sign -&gt; discordant\n        # If diff_x or diff_y == 0, that's a tie (we'll just mark it)\n        if diff_x * diff_y &gt; 0:\n            relation = \"concordant\"\n            C += 1\n        elif diff_x * diff_y &lt; 0:\n            relation = \"discordant\"\n            D += 1\n        else:\n            relation = \"tie\"\n\n        pairs_info.append({\n            \"Pair\": f\"{site_i}-{site_j}\",\n            f\"Order in {col_x}\": \"i&lt;j\" if diff_x &lt; 0 else \"i&gt;j\",\n            f\"Order in {col_y}\": \"i&lt;j\" if diff_y &lt; 0 else \"i&gt;j\",\n            \"Relation\": relation\n        })\n\n    pairs_df = pd.DataFrame(pairs_info)\n    return pairs_df, C, D\n\n# Now let's use the function on our sample data\npairs_df, C, D = kendall_concordance_table(data, \"Rank_Analyst1\", \"Rank_Analyst2\")\nprint(f\"Concordant pairs (C): {C}, Discordant pairs (D): {D}\") # print out the counts of concordant and discordant pairs\npairs_df # print the output table from the function: kendall_concordance_table()\n\nConcordant pairs (C): 13, Discordant pairs (D): 2\n\n\n\n\n\n\n\n\n\nPair\nOrder in Rank_Analyst1\nOrder in Rank_Analyst2\nRelation\n\n\n\n\n0\nA-B\ni&lt;j\ni&lt;j\nconcordant\n\n\n1\nA-C\ni&lt;j\ni&lt;j\nconcordant\n\n\n2\nA-D\ni&lt;j\ni&lt;j\nconcordant\n\n\n3\nA-E\ni&lt;j\ni&lt;j\nconcordant\n\n\n4\nA-F\ni&lt;j\ni&lt;j\nconcordant\n\n\n5\nB-C\ni&lt;j\ni&gt;j\ndiscordant\n\n\n6\nB-D\ni&lt;j\ni&lt;j\nconcordant\n\n\n7\nB-E\ni&lt;j\ni&lt;j\nconcordant\n\n\n8\nB-F\ni&lt;j\ni&lt;j\nconcordant\n\n\n9\nC-D\ni&lt;j\ni&lt;j\nconcordant\n\n\n10\nC-E\ni&lt;j\ni&lt;j\nconcordant\n\n\n11\nC-F\ni&lt;j\ni&lt;j\nconcordant\n\n\n12\nD-E\ni&lt;j\ni&lt;j\nconcordant\n\n\n13\nD-F\ni&lt;j\ni&lt;j\nconcordant\n\n\n14\nE-F\ni&lt;j\ni&gt;j\ndiscordant",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kendall rank correlation coefficient (Kendall's tau)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Kendalls_tau.html#compute-kendalls-tau-from-first-principles",
    "href": "Notebooks/Kendalls_tau.html#compute-kendalls-tau-from-first-principles",
    "title": "Kendall rank correlation coefficient (Kendall’s tau)",
    "section": "Compute Kendall’s Tau from first principles",
    "text": "Compute Kendall’s Tau from first principles\nNext we will compute Kendall’s tau manually using Python.\nremember that tau is the difference between the probability of concordant and discordant pairs.\nFor \\(n\\) items, the total number of distince pairs is given by the formula: \\(T = \\frac{n(n-1)}{2}\\).\nThen, we can use the output of the function above which counted the number of concordant and discordant pairs to compute kendall’s tau.\n\nimport numpy as np # for numerical operations we use numpy\n\nn = len(data) # number of items being ranked\ntotal_pairs = n * (n - 1) / 2 # total number of distinct pairs T\ntau_manual = (C - D) / total_pairs # Kendall's tau formula\n\nprint(f\" C = {C}, D = {D}, Total pairs (T): {total_pairs}\")\nprint(f\"Kendall's tau (manual calculation): {tau_manual:.3f}\")\n\n C = 13, D = 2, Total pairs (T): 15.0\nKendall's tau (manual calculation): 0.733\n\n\n\nInterpreting the results\n\nif tau is close to +1, the rankings mostly agree if tau is close to 0.5, mostly agree but with notable flips if tau is close to -1, the rankings mostly disagree if tau is close to -0.5, mostly disagree but with agreements if tau is close to 0, there is little association between the rankings\nFor this synthetic dataset we should see \\(\\tau\\) as high but not 1 because those B vs C and E vs F swaps create discordance.\n\nNext lets visualize the agreement and disagreement between the two rankings.\n\nTo do this we will plot…\n\nimport matplotlib.pyplot as plt\n\ncolor_map = { 'concordant': 'green', 'discordant': 'red', \"tie\": 'gray' }\n\nplt.figure(figsize=(10, 6))\npair_counts = pairs_df[\"Relation\"].value_counts()\nbars = plt.bar(pair_counts.index, pair_counts.values,\n               color=[color_map[r] for r in pair_counts.index])\n\nplt.ylabel(\"Number of Pairs\")\nplt.title(\"Counts of Concordant and Discordant Pairs\")\nplt.show()\n\n\n\n\n\n\n\n\nSo looking at the figure we can see that most of the pairs are concordant, we have a few discordant pairs (in red) and no ties (gray). We got a kendall’s tau of 0.733 which indicates a strong positive association between the two rankings.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kendall rank correlation coefficient (Kendall's tau)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Kendalls_tau.html#compare-manual-calculation-to-scipy.stats",
    "href": "Notebooks/Kendalls_tau.html#compare-manual-calculation-to-scipy.stats",
    "title": "Kendall rank correlation coefficient (Kendall’s tau)",
    "section": "Compare manual calculation to scipy.stats",
    "text": "Compare manual calculation to scipy.stats\nNow that we understand the basic calculation of Kendall’s tau, lets try to use the scipy.stats version of kendall’s tau to see if we get the same results and how our manual calculation compares to the built-in function in scipy.stats\n\nfrom scipy.stats import kendalltau # import kendalltau from scipy.stats\n\n# recall the structure of our data\nprint(data.columns) # show column names\ndata.head(5) # show first 5 rows of data\n\nIndex(['Site', 'Rank_Analyst1', 'Rank_Analyst2'], dtype='object')\n\n\n\n\n\n\n\n\n\nSite\nRank_Analyst1\nRank_Analyst2\n\n\n\n\n0\nA\n1\n1\n\n\n1\nB\n2\n3\n\n\n2\nC\n3\n2\n\n\n3\nD\n4\n4\n\n\n4\nE\n5\n6\n\n\n\n\n\n\n\n\ntau_scipy, p_value = kendalltau(data[\"Rank_Analyst1\"], data[\"Rank_Analyst2\"])\nprint(f\"Kendall's tau (scipy.stats): {tau_scipy:.3f}, p-value: {p_value:.3f}\")\n\nKendall's tau (scipy.stats): 0.733, p-value: 0.056\n\n\nNotice that both our manual calculation and scipy’s kendalltau function give the same result of approximately 0.733, confirming the correctness of our manual implementation. But the scipy function also provides a p-value for testing the hypothesis of no association (\\(\\tau = 0\\))\nSo the scipy version does two things:\n\nIt computes Kendall’s tau using an efficient algorithm measuring the strength of monotonic association between two rankings.\nIt provides a p-value for testing the null hypothesis that there is no association between the two rankings (i.e., \\(\\tau = 0\\)). A low p-value (typically &lt; 0.05) indicates that we can reject the null hypothesis and conclude that there is a statistically significant association between the rankings.\n\nHere we got a p-value of approximately 0.056, which indicates that the association is marginally significant at the 0.05 level. This suggests that while there is a positive association between the rankings, we should be cautious in interpreting it as statistically significant. Why? Because our sample dataset is small (only 5 items), with a such a small number of pairs its more likely that random chance could produce similar levels of concordance. lets see what happens when we increase the size of the dataset.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kendall rank correlation coefficient (Kendall's tau)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Kendalls_tau.html#adding-complexity",
    "href": "Notebooks/Kendalls_tau.html#adding-complexity",
    "title": "Kendall rank correlation coefficient (Kendall’s tau)",
    "section": "Adding complexity",
    "text": "Adding complexity\nTo further explore the behavior of Kendall’s tau, we can increase the size of our dataset from 6 sites to 30. We will randomly generate base ranking, then create a slightly “noisy” version to simulate small differences in judgement or weight perturbations.\n\nnp.random.seed(32)  # for reproducibility\nn = 100 # change this and re-run as well to see the effect of sample size \nswap_n = 30 # number of swaps to introduce, change this value and re-run to see different levels of disagreement\n\n# Analyst A: perfect ranking 1 -&gt; n\nrank_A = np.arange(1, n + 1) # Analyst A ranks items from 1 to n\n\n# Analyst B: same order bit with some random swaps (simulateing disagreement)\nrank_B = rank_A.copy() # start with same ranking as Analyst A\nswap_indices = np.random.choice(n, size=swap_n, replace=False) # choose 5 random indices to swap\nnp.random.shuffle(swap_indices) # shuffle the selected indices\nrank_B[swap_indices] = rank_B[np.random.permutation(swap_indices)]  # perform the swaps\n\ntau, p_value = kendalltau(rank_A, rank_B)\nprint(f\"Kendall's tau between Analyst A and B: {tau:.3f}, p-value: {p_value:.3f}\")\n\n# Visualize the rankings in a scatter plot\nplt.figure(figsize=(5,5))\nplt.scatter(rank_A, rank_B)\nplt.plot([0,n],[0,n],'k--',alpha=0.5)\nplt.xlabel(\"Analyst A rank\")\nplt.ylabel(\"Analyst B rank\")\nplt.title(f\"n = {n}, τ = {tau:.3f}\")\nplt.show()\n\n\nKendall's tau between Analyst A and B: 0.593, p-value: 0.000\n\n\n\n\n\n\n\n\n\n\nNext lets add noise a different way.\n\nFirst we will create a set of data, we will call base_scores we will just take \\(n\\) numbers spaced equally from 0 to 1\nthen we will create alternative scores which are the base_scores with some noise added, noise from a random normal distribution.\n\n\nn = 19 # change this depending on the sample size you want\nnoise_factor = 0.08 # this is the standard devation of the distribution from which the noise is generated, try 0.1, 0.05, 1, see how this affects kendalls tau\nbase_scores = np.linspace(0,1,n)\n#print(f\"base_scores {base_scores}\")\n\nnoise = np.random.normal (0,noise_factor,n) # create noise by drawing random samples from a normal gaussian distribution\n\nalt_scores = base_scores + noise # add the noise to the base scores to create alternative scores\n#print(f\"alt scores (base scores + noise) {alt_scores}\")\n\nrank_base = pd.Series(base_scores).rank() # rank the original scores, (remember we want ranks not scores)\nrank_alt = pd.Series(alt_scores).rank() # the base scores have been changed a bit randomly so they will rank differently when their rank is calculated\n\n# now lets calculate kendall's tau\n\ntau, p_value = kendalltau(rank_base, rank_alt)\nprint(f\"τ = {tau:.3f}; pvalue = {p_value:.5f}\")\n\nτ = 0.860; pvalue = 0.00000\n\n\n\n# plot the relationship between rank_base, and rank_alt (our two different rankings)\n\nplt.figure(figsize=(6,4))\nplt.scatter(base_scores, alt_scores)\nplt.xlabel(\"Suitability A\")\nplt.ylabel(\"Suitability B (perturbed)\")\nplt.title(f\"Kendall’s τ = {tau:.3f}\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLets now look at how changing the level of noise in the data affects the kendalls tau\n\nWe will generate many random pertubations and compute \\(\\tau\\) each time. This mimics the way sensitivity analysis samples random weight combinations in a Weighted Linear Combination (WLC). basically we are looking at the kendall’s tau through time, and automating the changing of the standard devation within the noise addition to see how adding different levels of noise affects kendall’s tau.\n\n\nnoise_levels = np.linspace(0,0.5,20)\n#print(noise_levels)\n\ntaus = []\n\nfor s in noise_levels:\n    alt = base_scores + np.random.normal(0 , s , n) # recall n is defined above in previous example as is base_scores\n    taus.append(kendalltau(pd.Series(base_scores).rank(), pd.Series(alt).rank())[0])\n\n\nplt.plot(noise_levels, taus, marker='o')\nplt.xlabel(\"Noise (std dev)\")\nplt.ylabel(\"Kendall’s τ\")\nplt.title(\"Rank stability vs. perturbation level\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nyou can see from the above figure that as noise increases kendall’s tau a measure of similarity between rankings decreases, rank stability decreases.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kendall rank correlation coefficient (Kendall's tau)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Kendalls_tau.html#real-world-example---countries-ranked-by-life-expectency-and-gdp",
    "href": "Notebooks/Kendalls_tau.html#real-world-example---countries-ranked-by-life-expectency-and-gdp",
    "title": "Kendall rank correlation coefficient (Kendall’s tau)",
    "section": "real-world example - Countries Ranked by Life Expectency and GDP",
    "text": "real-world example - Countries Ranked by Life Expectency and GDP\nOk, enough with fake data sets, lets step away from the hard sciences for a second and look at something more social. Lets look at how life expectancy compares to GDP, we would think life expectancy is higher in rich countries and lower in poor countries. So we can get the data on life expectancy, and we can get the data on GDP, then rank the countries in order of GDP and life expectancy, and compare how these two different ways to rank countries are concordant or discordant using Kendalls Tau\n\nto see how we cleaned and created these datasets see this notebook: Data\\DataWranglingScripts\\GDPvLifeExpectency2022_countries_ranked.ipynb\n\n\ndf = pd.read_csv(\"../Data/CLEAN/GDP_LifeExpectancy_2022_Clean.csv\")\nprint(df.head())\nprint(df.columns)\n\n                  Country Name Country Code   GDP_PC_2022  LIFE_EX_YRS_2022  \\\n0                        Aruba          ABW  30559.533535         73.537000   \n1  Africa Eastern and Southern          AFE   1628.318944         61.765707   \n2                  Afghanistan          AFG    357.261153         63.941000   \n3   Africa Western and Central          AFW   1796.668633         56.906135   \n4                       Angola          AGO   2929.694455         61.748000   \n\n   GDP_PC_RANK_2022  LIFE_EX_YRS_RANK_2022  \n0              56.0                   86.0  \n1             216.0                  220.0  \n2             255.0                  203.0  \n3             210.0                  251.0  \n4             187.0                  221.0  \nIndex(['Country Name', 'Country Code', 'GDP_PC_2022', 'LIFE_EX_YRS_2022',\n       'GDP_PC_RANK_2022', 'LIFE_EX_YRS_RANK_2022'],\n      dtype='object')\n\n\n\ntau, pval = kendalltau(df[\"GDP_PC_RANK_2022\"], df[\"LIFE_EX_YRS_RANK_2022\"])\n\nprint(f\"Kendall's tau (τ) = {tau:.3f}\")\nprint(f\"p-value = {pval:.8f}\")\n\nKendall's tau (τ) = 0.651\np-value = 0.00000000\n\n\nWe see a strong positive rank correlation, which is what we would expect. Wealthier countries generally have longer life expectancy, though the relationship isnt perfect. Now lets visualize the data.\n\nplt.figure(figsize=(7,7))\nplt.scatter(\n    df[\"GDP_PC_RANK_2022\"],\n    df[\"LIFE_EX_YRS_RANK_2022\"],\n    alpha=0.7,\n    edgecolor=\"k\",\n    linewidth=0.3\n)\n\n# Add a diagonal \"perfect agreement\" line\nplt.plot(\n    [1, df[\"GDP_PC_RANK_2022\"].max()],\n    [1, df[\"LIFE_EX_YRS_RANK_2022\"].max()],\n    'k--', alpha=0.5, label=\"Perfect rank agreement\"\n)\n\nplt.xlabel(\"GDP per capita rank (1 = richest)\")\nplt.ylabel(\"Life expectancy rank (1 = longest-lived)\")\nplt.title(f\"Country Rank Agreement\\nKendall’s τ = {tau:.3f}\")\n\n# Flip axes so 'better' (rank 1) appears top-right\nplt.gca().invert_xaxis()\nplt.gca().invert_yaxis()\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNext we will zoom in on the top 40 countries in terms of GDP and see if the kendalls tau is better or worse when we exclude all but the 40 richest.\n\n# Sort by GDP rank (1 = richest)\ndf_top40 = (\n    df\n    .sort_values(\"GDP_PC_RANK_2022\", ascending=True)\n    .head(40)\n    .copy()\n)\n\nprint(df_top40[[\"Country Name\", \"GDP_PC_RANK_2022\", \"LIFE_EX_YRS_RANK_2022\"]].head())\nprint(f\"Number of countries in subset: {len(df_top40)}\")\n\n      Country Name  GDP_PC_RANK_2022  LIFE_EX_YRS_RANK_2022\n144         Monaco               1.0                    2.0\n133  Liechtenstein               2.0                    3.0\n140     Luxembourg               3.0                   14.0\n27         Bermuda               4.0                   42.0\n172         Norway               5.0                   11.0\nNumber of countries in subset: 40\n\n\n\ntau_top, pval_top = kendalltau(\n    df_top40[\"GDP_PC_RANK_2022\"],\n    df_top40[\"LIFE_EX_YRS_RANK_2022\"]\n)\n\nprint(f\"Kendall’s τ (top 40 richest) = {tau_top:.3f}\")\nprint(f\"p-value = {pval_top:.8f}\")\n\nKendall’s τ (top 40 richest) = 0.248\np-value = 0.02515424\n\n\n\ndf_top40[\"RankGap\"] = (\n    df_top40[\"GDP_PC_RANK_2022\"] - df_top40[\"LIFE_EX_YRS_RANK_2022\"]\n)\n\nplt.figure(figsize=(8,8))\nscatter = plt.scatter(\n    df_top40[\"GDP_PC_RANK_2022\"],\n    df_top40[\"LIFE_EX_YRS_RANK_2022\"],\n    c=df_top40[\"RankGap\"],\n    cmap=\"RdBu_r\",\n    s=70,\n    edgecolor=\"k\",\n    linewidth=0.4\n)\nplt.colorbar(scatter, label=\"Rank Gap (GDP − Life Exp)\")\n\nplt.plot(\n    [1, df_top40[\"GDP_PC_RANK_2022\"].max()],\n    [1, df_top40[\"LIFE_EX_YRS_RANK_2022\"].max()],\n    'k--', alpha=0.4\n)\n\nfor _, row in df_top40.iterrows():\n    plt.text(\n        row[\"GDP_PC_RANK_2022\"] + 0.2,\n        row[\"LIFE_EX_YRS_RANK_2022\"],\n        row[\"Country Name\"],\n        fontsize=8\n    )\n\nplt.xlabel(\"GDP per capita rank (1 = richest)\")\nplt.ylabel(\"Life expectancy rank (1 = longest-lived)\")\nplt.title(f\"Top 40 Wealthiest Countries: Over / Under-Performers\\nτ = {tau_top:.3f}\")\n\nplt.gca().invert_xaxis()\nplt.gca().invert_yaxis()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe analysis of country rank in terms of GDP per capita vs life expectancy using Kendall’s tau tells us that overall life expectency is correlated with GDP per capita in terms of how contries compare to eachother (rank), however the correlation is much worse in the rich countries, why might that be?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kendall rank correlation coefficient (Kendall's tau)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Kendalls_tau.html#conceptual-questions",
    "href": "Notebooks/Kendalls_tau.html#conceptual-questions",
    "title": "Kendall rank correlation coefficient (Kendall’s tau)",
    "section": "CONCEPTUAL QUESTIONS",
    "text": "CONCEPTUAL QUESTIONS\n\nIs Kendall’s Tau a good way to measure the correlation of these two variables?\n\n\nanswer: Its more approriate to look at how the Life Expectency values compare to the GDP per capita directly using Pearson’s R, which compares one number to another. Kendall’s Tau is for comparing the ranks, so the colored rank figure shows us that countries below the line, are under performing in terms of life expectancy vs GDP relative to their neighbors. It is also telling us the the GDP life expectency relationship breaks down at higher levels of GDP or is less meaningful.\n\n\nWhy might this relationship breakdown when subsetting the data to only the richest countries?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kendall rank correlation coefficient (Kendall's tau)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Kendalls_tau.html#spatial-rank-correlation",
    "href": "Notebooks/Kendalls_tau.html#spatial-rank-correlation",
    "title": "Kendall rank correlation coefficient (Kendall’s tau)",
    "section": "Spatial Rank Correlation",
    "text": "Spatial Rank Correlation\n\nUsing Kendalls Tau for suitability mapping sensitivity Analysis\n\nIn a Weighted Linear Combination (WLC) or other GIS-MCDA, you often generate suitability rasters under different weighting schemes, e.g.:\n\nScenario A: baseline weights (e.g., 40% slope, 30% soil, 30% rainfall)\nScenario B: modified weights (e.g., 30% slope, 40% soil, 30% rainfall)\n\nEach raster cell gets a suitability score. You can rank cells (1 = most suitable) for each scenario, then compute Kendall’s \\(\\tau\\) between the two rankings.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kendall rank correlation coefficient (Kendall's tau)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Kendalls_tau.html#simulated-mcda-suitability",
    "href": "Notebooks/Kendalls_tau.html#simulated-mcda-suitability",
    "title": "Kendall rank correlation coefficient (Kendall’s tau)",
    "section": "simulated MCDA suitability",
    "text": "simulated MCDA suitability\nWe will build two small 10x10 rasteres (100 cells):\nsuitability_A –&gt; Baseline Scenario suitability_B –&gt; slightly perturbed version (change one weight layer)\n\nThen flatted both to 1D arrays (each cell = one observation)\ncompute \\(\\tau\\) for the full map\nvisualize where ranks changed the most.\n\n\nnp.random.seed(42)\n\n# --- Step 1: create two synthetic suitability grids (values 1–10) ---\ngrid_size = 10\nsuitability_A = np.random.rand(grid_size, grid_size) * 9 + 1  # values in [1,10]\nsuitability_B = suitability_A + np.random.normal(0, 0.02, (grid_size, grid_size))  # add mild noise\nsuitability_B = np.clip(suitability_B, 1, 10)  # keep within same range\n\ndiff = suitability_A - suitability_B\n\n# --- Step 2: plot them side-by-side ---\nfig, axes = plt.subplots(1, 3, figsize=(10, 4))\n\nim1 = axes[0].imshow(suitability_A, cmap=\"YlGn\", vmin=1, vmax=10)\naxes[0].set_title(\"Scenario A — Baseline\")\naxes[0].axis(\"off\")\nplt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04, label=\"Suitability (1–10)\")\n\nim2 = axes[1].imshow(suitability_B, cmap=\"YlGn\", vmin=1, vmax=10)\naxes[1].set_title(\"Scenario B — Perturbed\")\naxes[1].axis(\"off\")\nplt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04, label=\"Suitability (1–10)\")\n\nim3 = axes[2].imshow(diff, cmap = 'RdBu', vmin = diff.min(), vmax = diff.max())\naxes[2].set_title(\"Difference (added noise)\")\naxes[2].axis('off')\nplt.colorbar(im3, ax=axes[2], fraction=0.046, pad = 0.04, label=\"Difference\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# --- Step 3: compute Kendall’s tau on the flattened ranks ---\nA_flat = suitability_A.flatten()\nB_flat = suitability_B.flatten()\n\nrank_A = pd.Series(A_flat).rank()\nrank_B = pd.Series(B_flat).rank()\n\ntau, pval = kendalltau(rank_A, rank_B)\nprint(f\"Overall spatial Kendall’s τ = {tau:.3f} -- Pvalue: {pval:.4f}\")\n\nOverall spatial Kendall’s τ = 0.997 -- Pvalue: 0.0000\n\n\n\nrank_diff = (rank_B - rank_A).values.reshape(grid_size, grid_size)\n\nplt.figure(figsize=(6,5))\nplt.imshow(rank_diff, cmap=\"BrBG\", vmin=rank_diff.min(), vmax=rank_diff.max())\nplt.colorbar(label=\"Rank difference (B − A)\")\nplt.title(\"Spatial distribution of rank changes\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kendall rank correlation coefficient (Kendall's tau)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Kendalls_tau.html#review-questions",
    "href": "Notebooks/Kendalls_tau.html#review-questions",
    "title": "Kendall rank correlation coefficient (Kendall’s tau)",
    "section": "Review Questions",
    "text": "Review Questions\n\nConceptual\n\nWhat type of relationship does Kendall’s rank correlation coefficient (\\(\\tau\\)) measure between two variables?\nIn what ways does Kendall’s \\(\\tau\\) differ from Pearson’s \\(r\\) correlation coefficient?\nWhat do we mean by a concordant pair and a discordant pair in the context of Kendall’s \\(\\tau\\)?\nWrite the general formula for Kendall’s \\(\\tau\\) in terms of the number of concordant (\\(C\\)) and discordant (\\(D\\)) pairs, and the total number of pairs (\\(T\\)).\nWhat does a \\(\\tau\\) value of \\(+1\\), \\(0\\), and \\(-1\\) indicate about the association between two rankings?\nWhy is Kendall’s \\(\\tau\\) considered a non-parametric statistic? What assumption does it avoid that Pearson’s \\(r\\) requires?\nExplain how tied ranks affect the calculation of Kendall’s \\(\\tau\\). What adjustments are sometimes made to account for ties?\nHow can Kendall’s \\(\\tau\\) be interpreted as a difference in probabilities between concordant and discordant pairs?\nWhat is the computational relationship between Kendall’s \\(\\tau\\) and Spearman’s \\(\\rho\\) in terms of how they treat rank differences?\nWhy might Kendall’s \\(\\tau\\) be preferred over Pearson’s \\(r\\) when comparing ordinal data such as survey responses or ranked preferences?\n\n\n\nApplied Interpretive\n\nSuppose two analysts rank five sites for groundwater recharge suitability. Analyst A’s and B’s rankings are nearly identical except for one swapped pair. Would you expect \\(\\tau\\) to be closer to 1, 0, or -1? Why?\nIf a scatter plot of ranks shows that higher values of one ranking generally correspond to higher values of the other but with some local reversals, what approximate range of \\(\\tau\\) would you expect?\n\n\n\nAnswers\n\n1. Kendall’s rank correlation coefficient (\\(\\tau\\)) measures the strength and direction of monotonic association between two ranked variables. It quantifies how consistently the order of one variable corresponds to the order of another.\n\n\n2. Kendall’s \\(\\tau\\) is based on the number of concordant and discordant pairs, whereas Pearson’s \\(r\\) measures linear association based on actual data values. Kendall’s \\(\\tau\\) is non-parametric and uses only rank order information.\n\n\n3. A concordant pair is one where the relative ordering of two items is the same in both rankings (both increasing or both decreasing). A discordant pair is one where the order is reversed between the two rankings.\n\n\n4. The general formula is: \\[\n\\tau = \\frac{C - D}{T} = \\frac{C - D}{\\frac{1}{2}n(n - 1)}\n\\] where \\(C\\) is the number of concordant pairs, \\(D\\) is the number of discordant pairs, and \\(T\\) is the total number of distinct pairs.\n\n\n5. \\(\\tau = +1\\) indicates perfect agreement (all pairs are concordant), \\(\\tau = 0\\) indicates no association (equal mix of concordant and discordant pairs), and \\(\\tau = -1\\) indicates perfect disagreement (all pairs are discordant).\n\n\n6. Kendall’s \\(\\tau\\) is non-parametric because it does not assume any specific distribution of the variables or linearity of their relationship. It relies only on the ordinal information of the ranks rather than their numeric values.\n\n\n7. Tied ranks reduce the number of distinct pairs that can be classified as concordant or discordant. Adjusted versions of Kendall’s \\(\\tau\\) (e.g., \\(\\tau_b\\)) include correction factors to handle ties in one or both rankings.\n\n\n8. Kendall’s \\(\\tau\\) can be expressed as a difference in probabilities: \\[\n\\tau = P(\\text{concordant}) - P(\\text{discordant})\n\\] meaning it represents how much more likely it is that two randomly chosen observations are ranked in the same order than in the opposite order.\n\n\n9. Spearman’s \\(\\rho\\) is based on the differences between ranks and approximates Pearson’s \\(r\\) computed on ranks, while Kendall’s \\(\\tau\\) directly counts pairwise order agreements. Both measure monotonic relationships, but Kendall’s \\(\\tau\\) is generally smaller in magnitude and has a clearer probabilistic interpretation.\n\n\n10. Kendall’s \\(\\tau\\) is preferred for ordinal data because it depends only on the relative ordering of observations, making it robust to nonlinearity and outliers. Pearson’s \\(r\\) assumes interval-scale data and linearity, which are often not valid for ranked responses.\n\n\n\n11. The value of \\(\\tau\\) would be close to \\(+1\\), since most pairs remain concordant and only one pair is discordant. A single reversal in a small dataset slightly lowers \\(\\tau\\) but does not change its sign.\n\n\n12. The value of \\(\\tau\\) would likely fall between \\(0.4\\) and \\(0.8\\), indicating a moderately strong positive monotonic association with some inconsistencies in the ordering.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kendall rank correlation coefficient (Kendall's tau)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html",
    "href": "Notebooks/Morris_Elementary_Effects.html",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "",
    "text": "Why was Morris developed?\nIn global sensitivity analysis, the Morris Method (also called the Method of Elementary Effects) is a screening technique used to identify which input variables in a model have the greatest influence on the output. It was introduced by Max D. Morris in 1991 as a computationally efficient way to explore sensitivity in models with many inputs, without requiring an enormous number of model runs.\nThe basic idea is:\nThat small one-at-a-time change in an input and the resulting change in the model output is called an elementary effect.\nBefore Morris (1991), sensitivity analysis often lived in two extremes:\nMax Morris was looking for something in-between:\nSo the Morris method is often described as a screening method: it’s a first pass that tells you which inputs matter and how they matter, so you know where to focus more detailed analysis later.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html#why-was-morris-developed",
    "href": "Notebooks/Morris_Elementary_Effects.html#why-was-morris-developed",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "",
    "text": "Local / derivative-based sensitivity:\nThis asks “If I make a tiny change to one parameter around the current baseline, how much does the output change?”\nThis is basically a partial derivative.\nProblem: it only tells you about behavior near one point in parameter space, and it assumes smooth / linear behavior.\nFull global variance-based methods (like Sobol indices):\nThese methods try to quantify how much of the total output variance is explained by each input and by their interactions.\nThey’re extremely informative — but also computationally expensive, because they require a lot of model evaluations.\n\n\n\nA method that is global (explores the whole parameter space, not just one point),\nBut still cheap enough to run early, even for high-dimensional problems (many inputs),\nAnd able to flag inputs that are likely important, nonlinear, or interacting.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html#what-problems-does-the-morris-method-solve",
    "href": "Notebooks/Morris_Elementary_Effects.html#what-problems-does-the-morris-method-solve",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "What problems does the Morris method solve?",
    "text": "What problems does the Morris method solve?\nImagine you have a model:\n\\[\ny = f(x_1, x_2, x_3, \\dots, x_k)\n\\]\nwhere each \\(x_i\\) is an input (a criterion weight, a threshold, a soil parameter, etc.), and \\(y\\) is some decision score (e.g. total suitability, predicted recharge, contaminant load, habitat score).\nYou want to know:\n\nWhich inputs have basically no effect on \\(y\\)? (Those might be safely fixed or ignored.)\nWhich inputs have a large overall effect on \\(y\\)? (These are important drivers.)\nWhich inputs behave nonlinearly or interact with other inputs?\n(For example, “slope only matters once soil permeability is high,” or “forest cover and precipitation together change infiltration potential in a way you don’t get by looking at either alone.”)\n\nThe Morris method gives you exactly that information with two summary statistics per input.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html#core-concept-the-elementary-effect",
    "href": "Notebooks/Morris_Elementary_Effects.html#core-concept-the-elementary-effect",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "Core concept: the Elementary Effect",
    "text": "Core concept: the Elementary Effect\nFor each input \\(x_i\\), we define an elementary effect as:\n\\[\nEE_i = \\frac{f(x_1, \\dots, x_i + \\Delta, \\dots, x_k) - f(x_1, \\dots, x_i, \\dots, x_k)}{\\Delta}\n\\]\nWhere:\n\n\\(\\Delta\\) is a small step in the value of \\(x_i\\),\nAll other inputs are held constant for that step,\nThe numerator is “how much the output changed when we nudged just \\(x_i\\).”\n\nInterpretation:\n\n\\(EE_i\\) is basically: “If I change only \\(x_i\\) a little, how much does the model output respond?”\n\nBut — and this is the key difference from local sensitivity — we don’t do this just once from one baseline. We repeat this from multiple random locations in the input space. Each repetition gives us another possible elementary effect for that same input.\nSo for each input \\(x_i\\), we don’t just get one number. We get a distribution of elementary effects across the space of plausible inputs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html#morris-summary-metrics",
    "href": "Notebooks/Morris_Elementary_Effects.html#morris-summary-metrics",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "Morris summary metrics",
    "text": "Morris summary metrics\nAfter computing many elementary effects for each input, we summarize them. The two most common summaries are:\n\n\\(\\mu^*\\) (mu star):\nThe mean of the absolute value of the elementary effects for that input.\n\nHigh \\(\\mu^*\\) means: changing this input tends to cause a big change in the output overall.\n\nThis is interpreted as “overall importance” or “influence strength.”\n\nWe use the absolute value so positive and negative effects don’t cancel each other out.\n\\(\\sigma\\) (sigma):\nThe standard deviation of the elementary effects for that input.\n\nHigh \\(\\sigma\\) means: the effect of this input is not consistent — sometimes it has a big effect, sometimes small, sometimes positive, sometimes negative.\n\nThat usually indicates nonlinearity or interactions with other inputs.\n\nIntuition: if an input only mattered under certain combinations of other inputs, you’d see a wide spread in its elementary effects → high σ.\n\nThis gives you a beautiful diagnostic plot: μ* on the x-axis (importance) vs σ on the y-axis (interaction / nonlinearity).\n\nInputs with low μ* and low σ → mostly irrelevant.\nInputs with high μ* and low σ → consistently important, mostly linear effect on the output.\nInputs with high μ* and high σ → important but tricky: nonlinear or involved in interactions.\n\nThat’s the classic “Morris scatter plot.”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html#how-it-works-conceptually",
    "href": "Notebooks/Morris_Elementary_Effects.html#how-it-works-conceptually",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "How it works (conceptually)",
    "text": "How it works (conceptually)\n\nDefine ranges (or distributions) for each input \\(x_i\\).\nExample: slope weight in WLC could vary from 0.1 to 0.4, precipitation weight from 0.2 to 0.6, etc.\nSample a sequence of points in that input space (called “trajectories” or “paths”).\nEach path walks through the space one input at a time, changing one variable by \\(\\Delta\\) while keeping the others fixed, then moving on to the next variable, etc.\nFor each step along that path, compute the elementary effect \\(EE_i\\).\nAggregate all the elementary effects for each input across all paths → get \\(μ^*\\) and \\(\\sigma\\).\nRank or plot inputs based on \\(\\mu^*\\) and σ to decide which inputs matter.\n\nThis is global because you’re sampling across the full allowable range of inputs — not just perturbing around a single baseline point.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html#why-this-is-used",
    "href": "Notebooks/Morris_Elementary_Effects.html#why-this-is-used",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "Why this is used",
    "text": "Why this is used\nThe Morris method is widely used in:\n\nEnvironmental modeling and hydrology (e.g., identifying which hydrogeologic parameters most influence recharge estimates or contaminant transport),\nEcological and habitat suitability modeling,\nGroundwater recharge / infiltration models,\nFlood and erosion models,\nMulti-criteria decision analysis (MCDA), including GIS-based suitability mapping, to see which criteria weights dominate the final suitability score, and where there are strong interactions.\n\nIn practice:\n\nYou run Morris first to screen out unimportant variables (so you don’t waste computation on them),\nThen you apply heavier methods (like Sobol variance decomposition) on the variables that survived screening.\n\nSo Morris is both:\n\nA science tool (which parameters actually matter?),\nA workflow tool (where should I spend my expensive computation time?).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html#summary",
    "href": "Notebooks/Morris_Elementary_Effects.html#summary",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "Summary",
    "text": "Summary\n\nThe Morris method is a global, one-factor-at-a-time sensitivity screening method introduced by Max D. Morris in 1991.\nIt’s built around elementary effects: the change in model output when you nudge one input while holding others constant.\nBy repeating that across many starting points, you get a distribution of effects for each input.\nYou then summarize each input with:\n\n\\(\\mu^*\\) (mean absolute elementary effect): how influential this input is overall,\n\\(\\sigma\\) (stdev of elementary effects): how nonlinear or interaction-heavy its influence is.\n\nThis is incredibly helpful in decision-support models (like WLC suitability mapping) because it tells you:\n\nwhich criteria weights dominate suitability,\nwhich ones only matter in combination,\nand which ones are basically irrelevant.\n\n\n\nNext, we’ll:\n\nBuild a tiny synthetic model in Python so you can see elementary effects for a toy function,\nCompute \\(\\mu^*\\) and \\(\\sigma\\) for each input manually,\nReproduce what SALib’s morris routines do,\nVisualize \\(\\mu^*\\) vs \\(\\sigma\\),\nThen connect that to a spatial WLC / MCDA setting.\n\n\nNow let’s start generating elementary effects for a simple model in Python.\n\n\n## Import libraries and set random seed for repeatability \n\nimport numpy as np  \nimport pandas as pd\nimport matplotlib.pyplot as plt \n\n\nnp.random.seed(12)\n\n\nLets define a sample model\n\nWe want the model to have the following attributes:\n\ndepends on multiple inputs\nis at least a little nonlinear\nhas some interactions\n\n\\[\nf(x_1, x_2, x_3) = 2x_1 + 0.5x^2_2 + 3x_1 x_3\n\\]\nfeatures of this function:\n\n\\(2x_1\\) linear effect of \\(x_1\\)\n\\(0.5x^2_2\\) nonlinear effect of \\(x_2\\)\n\\(3x_1 x_3\\) interaction between \\(x_1\\) and \\(x_3\\)\n\n\n# function to test for elementary effects\n\ndef model(x1,x2,x3):\n    return 2*x1 + 0.5*(x2**2) + 3*x1*x3\n\n\nDefine and measure elementary effects\n\nFor a given input \\(x_i\\) the elementary effect is:\n\\[\nEE_i = \\frac{f(...,x_i + \\Delta,...)-f(...,x_i,...))}{\\Delta}\n\\]\nWe will create a function that does the following:\n\npick a random starting point \\((x_i)\\) from \\((x_1,x_2,x_3)\\)\nnudge just one variable by \\(\\Delta\\)\ncompute the change in output per unit step\n\n\nnotes: 1. we clip the perturbed value so we stay in a valid range [0,1]. 2. we gaurd the denominator in case delta pushes us out and gets clipped to the same value.\n\n\ndef elementary_effect(model_func, x, i, delta):\n    \"\"\"\n    compute the elementary effect of variable i at point x.\n\n    model_func: callable f(x1,x2,x3)\n    x: np.array shape (3,) representing [x1,x2,x3]\n    i: which index to perturb (0,1,2)\n    delta: step sie to add to x[i]\n\n    returns: EE_i float\n    \"\"\"\n    x_base = x.copy()\n    x_perturbed = x.copy()\n    x_perturbed[i] = x_perturbed[i] + delta\n    x_perturbed[i] = np.clip(x_perturbed[i], 0, 1)\n\n    y0 = model_func(*x_base)\n    y1 = model_func(*x_perturbed)\n\n    return (y1 - y0) / (x_perturbed[i] - x_base[i] + 1e-12)\n\n\n\nSample multiple points in input space and gather EEs\n\nWe will:\n\ndraw random points in [0,1]^3,\nfor each point, compute EEs for x1, x2, x3,\nrepeat for, say, 50 random points.\n\n\ndef sample_elementary_effects(model_func, n_samples=50, delta=0.1):\n    \"\"\"\n    For n_samples random base points in [0,1]^3,\n    compute elementary effects for each of the 3 inputs.\n\n    Returns: DataFrame with columns:\n      ['x1_EE', 'x2_EE', 'x3_EE']\n    \"\"\"\n    records = []\n\n    for _ in range(n_samples):\n        # random point in [0,1]^3\n        x = np.random.rand(3)\n\n        ee_x1 = elementary_effect(model_func, x, i=0, delta=delta)\n        ee_x2 = elementary_effect(model_func, x, i=1, delta=delta)\n        ee_x3 = elementary_effect(model_func, x, i=2, delta=delta)\n\n        records.append({\n            \"x1_EE\": ee_x1,\n            \"x2_EE\": ee_x2,\n            \"x3_EE\": ee_x3,\n        })\n\n    return pd.DataFrame(records)\n\nee_df = sample_elementary_effects(model, n_samples=50, delta=0.1)\nee_df.head()\n\n\n\n\n\n\n\n\nx1_EE\nx2_EE\nx3_EE\n\n\n\n\n0\n2.789945\n0.790050\n0.462489\n\n\n1\n4.756241\n0.064575\n1.601218\n\n\n2\n4.870848\n0.083421\n2.702145\n\n\n3\n3.818250\n0.333828\n0.411628\n\n\n4\n2.006778\n0.902736\n2.832675\n\n\n\n\n\n\n\n\nNext we compute \\(\\mu^*\\) and \\(\\sigma\\)\n\n\nsummary = pd.DataFrame({\n    \"mu_star\": [\n        ee_df[\"x1_EE\"].abs().mean(),\n        ee_df[\"x2_EE\"].abs().mean(),\n        ee_df[\"x3_EE\"].abs().mean()\n    ],\n    \"sigma\": [\n        ee_df[\"x1_EE\"].std(ddof=1),\n        ee_df[\"x2_EE\"].std(ddof=1),\n        ee_df[\"x3_EE\"].std(ddof=1)\n    ]\n}, index=[\"x1\", \"x2\", \"x3\"])\n\nsummary\n\n\n\n\n\n\n\n\nmu_star\nsigma\n\n\n\n\nx1\n3.378779\n0.940584\n\n\nx2\n0.554154\n0.283397\n\n\nx3\n1.480260\n0.861859\n\n\n\n\n\n\n\n\nplot \\(\\mu^*\\) vs \\(\\sigma\\)\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6,5))\n\n# --- base scatter ---\nplt.scatter(summary[\"mu_star\"], summary[\"sigma\"],\n            s=100, color=\"teal\", edgecolor=\"k\")\n\n# --- reference lines (mean μ★ and mean σ) ---\nmu_mean = summary[\"mu_star\"].mean()\nsigma_mean = summary[\"sigma\"].mean()\n\nplt.axvline(mu_mean, color=\"gray\", linestyle=\"--\", alpha=0.6,\n            label=f\"mean μ* = {mu_mean:.2f}\")\nplt.axhline(sigma_mean, color=\"gray\", linestyle=\"--\", alpha=0.6,\n            label=f\"mean σ = {sigma_mean:.2f}\")\n\n# --- labels for each variable ---\nfor var_name in summary.index:\n    plt.text(summary.loc[var_name, \"mu_star\"] + 0.05,\n             summary.loc[var_name, \"sigma\"] + 0.02,\n             var_name, fontsize=10)\n\n# --- axes & title ---\nplt.xlabel(r\"$\\mu^*$ (overall influence)\")\nplt.ylabel(r\"$\\sigma$ (nonlinearity / interaction)\")\nplt.title(\"Morris Elementary Effects – Screening Plot\")\nplt.legend(loc=\"upper left\", frameon=False)\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ninterpretation:\nThe graph shows the following:\n\n\\(x_2\\) has low influence and low interaction\n\\(x_1\\) has high interaction and high influence\n\\(x_3\\) has high interaction but low influence\n\nThis makes sense looking at our model:\n\\[\nf(x_1, x_2, x_3) = 2x_1 + 0.5x^2_2 + 3x_1 x_3\n\\]\nfeatures of this function:\n\n\\(2x_1\\) linear effect of \\(x_1\\)\n\\(0.5x^2_2\\) small but nonlinear effect of \\(x_2\\)\n\\(3x_1 x_3\\) interaction between \\(x_1\\) and \\(x_3\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html#goal",
    "href": "Notebooks/Morris_Elementary_Effects.html#goal",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "Goal",
    "text": "Goal\nWe’ll use the Morris method to analyze which morphological features most influence a penguin’s body mass.\nSpecifically: 1. Inputs (factors):\n- Bill length (mm)\n- Bill depth (mm)\n- Flipper length (mm)\n2. Output (model response):\n- Body mass (g)\nWe’ll fit a simple regression model ( f(x_1, x_2, x_3) ) that predicts body mass from these inputs, then treat this model as our “black box.”\nAfterward, we’ll apply the Morris Elementary Effects method to quantify: - ( ^* ) → the overall (average) influence of each input, and\n- ( ) → the variability or nonlinearity of that influence across the input space.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html#why-this-example-works",
    "href": "Notebooks/Morris_Elementary_Effects.html#why-this-example-works",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "Why this example works",
    "text": "Why this example works\n\nInterpretability: It’s easy to reason about which traits should matter (flipper length and bill length should correlate with mass).\n\nDimensional simplicity: With only 3 numeric inputs, the results are easy to visualize in 2D (μ★–σ plot).\n\nInteraction potential: The relationships are not purely linear — for instance, the effect of flipper length might depend on species or bill size.\n\n\n\nBy working with this dataset, we can see how the Morris method distinguishes between strong, consistent influences (high μ★, low σ) and context-dependent, interacting ones (high μ★, high σ), even in an everyday biological system.\n\n\nimport seaborn as sns\nimport pandas as pd\n\n# Load dataset\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\n\n# keep only columns we need and drop NAs\n\ncols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]\n\ndf = penguins[cols].dropna().copy()\ndf.head(), df.shape\n\n(   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n 0            39.1           18.7              181.0       3750.0\n 1            39.5           17.4              186.0       3800.0\n 2            40.3           18.0              195.0       3250.0\n 4            36.7           19.3              193.0       3450.0\n 5            39.3           20.6              190.0       3650.0,\n (342, 4))\n\n\n\nfit simple regression model\nWe’ll use scikit-learns linear regression as our block-box model.\nThis gives us \\(\\hat{y} = f(x_1,x_2,x_3)\\) that we can later probe with Morris.\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nX = df[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]]\ny = df[\"body_mass_g\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nprint(\"R^2 on train:\", model.score(X_train, y_train))\nprint(\"R^2 on test :\", model.score(X_test, y_test))\nmodel.coef_, model.intercept_\n\nR^2 on train: 0.7694159737586672\nR^2 on test : 0.7210757528501677\n\n\n(array([ 7.12002238,  7.65512529, 48.21508216]),\n np.float64(-5931.683059218843))\n\n\n\n\nwrap fitted model as function\nMorris neds a function that takes inputs and returns a scalar output.\nWe’ll define a function that:\n\naccepts three inputs(bill_length, bill_depth, flipper_length),\npacks them into the shape scikit-learn expects\nreturns the predicted body mass in grams\n\n\ndef penguin_mass_model(bill_length_mm, bill_depth_mm, flipper_length_mm):\n    \"\"\"\n    Predict penguin body mass (g) from morphology using our trained linear model.\n    Inputs are scalars.\n    Returns a scalar prediction (grams).\n    \"\"\"\n    X_input = pd.DataFrame([{\n        \"bill_length_mm\": bill_length_mm,\n        \"bill_depth_mm\": bill_depth_mm,\n        \"flipper_length_mm\": flipper_length_mm\n    }])\n    y_pred = model.predict(X_input)\n    return float(y_pred[0])\n\n\n\ndefine realistic ranges for each input\nto run morris, we need to tell it what ranges each input can take.\nWe’ll base that on the observed min/max in the data\n\nfeature_ranges = {\n    \"bill_length_mm\": (\n        df[\"bill_length_mm\"].min(),\n        df[\"bill_length_mm\"].max()\n    ),\n    \"bill_depth_mm\": (\n        df[\"bill_depth_mm\"].min(),\n        df[\"bill_depth_mm\"].max()\n    ),\n    \"flipper_length_mm\": (\n        df[\"flipper_length_mm\"].min(),\n        df[\"flipper_length_mm\"].max()\n    )\n}\n\nfeature_ranges\n\n{'bill_length_mm': (np.float64(32.1), np.float64(59.6)),\n 'bill_depth_mm': (np.float64(13.1), np.float64(21.5)),\n 'flipper_length_mm': (np.float64(172.0), np.float64(231.0))}\n\n\n\n# --- 1. Define the sampling and EE computation function ---\ndef sample_elementary_effects_real_model(model_func, feature_ranges, n_samples=100, delta=0.05):\n    \"\"\"\n    Compute elementary effects for each continuous input variable in the model.\n    \n    Parameters\n    ----------\n    model_func : callable\n        A function that takes three inputs (bill_length, bill_depth, flipper_length)\n        and returns a scalar prediction (body mass in g).\n    feature_ranges : dict\n        Dictionary mapping feature names to (min, max) tuples.\n    n_samples : int\n        Number of random points to sample.\n    delta : float\n        Fractional perturbation of each variable (e.g., 0.05 = 5% of its range).\n    \"\"\"\n    features = list(feature_ranges.keys())\n    records = []\n\n    for _ in range(n_samples):\n        # Randomly sample a base point inside the observed feature space\n        base_point = {}\n        for feat, (low, high) in feature_ranges.items():\n            base_point[feat] = np.random.uniform(low=low + 0.05*(high-low), high=high - 0.05*(high-low))\n        \n        # Baseline prediction\n        y0 = model_func(base_point[features[0]], base_point[features[1]], base_point[features[2]])\n\n        ee_point = {}\n        for feat in features:\n            low, high = feature_ranges[feat]\n            step = delta * (high - low)\n\n            perturbed_point = base_point.copy()\n            perturbed_point[feat] = np.clip(base_point[feat] + step, low, high)\n\n            y1 = model_func(\n                perturbed_point[features[0]],\n                perturbed_point[features[1]],\n                perturbed_point[features[2]]\n            )\n\n            ee_point[f\"{feat}_EE\"] = (y1 - y0) / step\n\n        records.append(ee_point)\n\n    return pd.DataFrame(records)\n\n# --- 2. Run the sampling ---\nee_df = sample_elementary_effects_real_model(\n    penguin_mass_model,\n    feature_ranges,\n    n_samples=100,\n    delta=0.05\n)\n\nee_df.head()\n\n\n\n\n\n\n\n\nbill_length_mm_EE\nbill_depth_mm_EE\nflipper_length_mm_EE\n\n\n\n\n0\n7.120022\n7.655125\n48.215082\n\n\n1\n7.120022\n7.655125\n48.215082\n\n\n2\n7.120022\n7.655125\n48.215082\n\n\n3\n7.120022\n7.655125\n48.215082\n\n\n4\n7.120022\n7.655125\n48.215082\n\n\n\n\n\n\n\n\nsummary = pd.DataFrame({\n    \"mu_star\": [ee_df[c].abs().mean() for c in ee_df.columns],\n    \"sigma\": [ee_df[c].std(ddof=1) for c in ee_df.columns]\n}, index=[c.replace(\"_EE\", \"\") for c in ee_df.columns])\n\nsummary\n\n\n\n\n\n\n\n\nmu_star\nsigma\n\n\n\n\nbill_length_mm\n7.120022\n6.091025e-13\n\n\nbill_depth_mm\n7.655125\n2.030373e-12\n\n\nflipper_length_mm\n48.215082\n3.332289e-13\n\n\n\n\n\n\n\n\nprint(summary)\nprint()\n\nprint(\"mu_star stats:\")\nprint(\" min:\", summary[\"mu_star\"].min())\nprint(\" max:\", summary[\"mu_star\"].max())\nprint(\" mean:\", summary[\"mu_star\"].mean())\n\nprint(\"\\nsigma stats:\")\nprint(\" min:\", summary[\"sigma\"].min())\nprint(\" max:\", summary[\"sigma\"].max())\nprint(\" mean:\", summary[\"sigma\"].mean())\n\nprint(\"\\nAny NaN in summary?\")\nprint(summary.isna().any())\n\n                     mu_star         sigma\nbill_length_mm      7.120022  6.091025e-13\nbill_depth_mm       7.655125  2.030373e-12\nflipper_length_mm  48.215082  3.332289e-13\n\nmu_star stats:\n min: 7.120022379018935\n max: 48.21508215626039\n mean: 20.996743274284498\n\nsigma stats:\n min: 3.3322894109700684e-13\n max: 2.030373237990764e-12\n mean: 9.909015515269688e-13\n\nAny NaN in summary?\nmu_star    False\nsigma      False\ndtype: bool\n\n\n\n\n\nmu_vals = summary[\"mu_star\"].values.astype(float)\nsigma_vals = summary[\"sigma\"].values.astype(float)\nnames = summary.index.tolist()\n\nmu_mean = float(np.mean(mu_vals))\nsigma_mean = float(np.mean(sigma_vals))\n\n# define plotting limits with a small % padding around actual data\nmu_min = mu_vals.min()\nmu_max = mu_vals.max()\nsigma_min = sigma_vals.min()\nsigma_max = sigma_vals.max()\n\nmu_pad = 0.1 * (mu_max - mu_min if mu_max &gt; mu_min else 1.0)\nsigma_pad = 0.1 * (sigma_max - sigma_min if sigma_max &gt; sigma_min else 1.0)\n\nx_lo = mu_min - mu_pad\nx_hi = mu_max + mu_pad\ny_lo = sigma_min - sigma_pad\ny_hi = sigma_max + sigma_pad\n\nfig, ax = plt.subplots(figsize=(6,5))\n\n# scatter points\nax.scatter(mu_vals, sigma_vals,\n           s=100, color=\"teal\", edgecolor=\"k\", zorder=3)\n\n# adaptive label offset: 2% of axis span instead of hardcoded 0.02\nx_offset = 0.02 * (x_hi - x_lo)\ny_offset = 0.02 * (y_hi - y_lo)\n\nfor x, y, label in zip(mu_vals, sigma_vals, names):\n    ax.text(x + x_offset, y + y_offset, label,\n            fontsize=10, zorder=4)\n\n# reference lines\nax.axvline(mu_mean, color=\"gray\", linestyle=\"--\", alpha=0.6,\n           label=f\"mean μ* = {mu_mean:.2f}\")\nax.axhline(sigma_mean, color=\"gray\", linestyle=\"--\", alpha=0.6,\n           label=f\"mean σ = {sigma_mean:.2e}\")\n\n# set sane limits\nax.set_xlim(x_lo, x_hi)\nax.set_ylim(y_lo, y_hi)\n\nax.set_xlabel(\"μ* (overall influence on predicted mass)\")\nax.set_ylabel(\"σ (interaction / nonlinearity)\")\nax.set_title(\"Morris Sensitivity Screening — Penguin Body Mass Model\")\nax.grid(alpha=0.3)\nax.legend(loc=\"upper right\", frameon=False)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nInterpreting the Morris screening for penguin mass\nWe used a linear regression model to predict penguin body mass (g) from bill length, bill depth, and flipper length. We then applied the Morris Elementary Effects method to measure how sensitive the predicted mass is to each morphological input.\nResults: - flipper_length_mm has the highest μ, meaning it is the most influential predictor of body mass overall. Small changes in flipper length lead to large changes in predicted body mass. - bill_length_mm and bill_depth_mm also affect predicted mass, but less strongly (their μ values are much smaller than flipper length). - All three inputs have σ values that are ~0. This means the effect of each input is essentially constant across the range of values we sampled. In other words, the model’s response to each input is linear and does not depend much on the other inputs.\nThis is exactly what we’d expect from a plain linear regression with no interaction terms: each predictor contributes additively and with a roughly constant slope. Because σ is near zero, we see no evidence of strong interactions or nonlinear behavior in this model.\nIn a more complex ecological model (or in a spatial MCDA with nonlinear suitability thresholds), we would expect to see higher σ, which would indicate that certain inputs matter only under certain conditions or in combination with other inputs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html#spatial-sensitivity-analysis-with-synthetic-rasters",
    "href": "Notebooks/Morris_Elementary_Effects.html#spatial-sensitivity-analysis-with-synthetic-rasters",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "Spatial Sensitivity Analysis with Synthetic Rasters",
    "text": "Spatial Sensitivity Analysis with Synthetic Rasters\nNow that we understand how Morris’ elementary effects method works conceptually, let’s explore how it applies in a spatial MCDA (Multi-Criteria Decision Analysis) setting — the kind used in suitability or recharge mapping.\nIn a spatial context, our “model” is often a weighted linear combination (WLC) of several raster criteria:\n\nslope\nsoil permeability\nrainfall\nland cover, etc.\n\nEach criterion is spatially continuous, and we assign weights to express their relative importance. The Morris method lets us vary these weights systematically to see:\n\nwhich weights most strongly influence the overall suitability outcome (μ★), and\nwhich weights interact in nonlinear ways (σ &gt; 0).\n\nTo demonstrate this, we’ll create a synthetic dataset of rasters that mimics realistic environmental layers and intentionally includes nonlinearity and interactions.\nOur synthetic study area will include:\n\nslope — higher toward one corner (representing uplands),\npermeability — highest in a central “valley,”\nrainfall — decreasing west to east, with sinusoidal north–south variability.\n\nWe’ll then define four “factors” that influence a hypothetical recharge suitability score:\n\n\n\n\n\n\n\n\nSymbol\nDescription\nBehavior\n\n\n\n\nwₛₗₒₚₑ\nWeight on slope suitability\nfavors gentle slopes (nonlinear decay)\n\n\nwₚₑᵣₘ\nWeight on soil permeability\nroughly linear (more permeable → better)\n\n\nwᵣₐᵢₙ\nWeight on rainfall suitability\n“Goldilocks” response — midrange rainfall best\n\n\nwᵢₙₜ\nWeight on interaction term\ncaptures synergy between rainfall and permeability\n\n\n\nWe’ll define a composite suitability model:\n\\[\n\\text{Suitability}(i,j) =\nw_\\text{slope},f_\\text{slope}(s_{ij}) +\nw_\\text{perm},f_\\text{perm}(p_{ij}) +\nw_\\text{rain},f_\\text{rain}(r_{ij}) +\nw_\\text{int},f_\\text{int}(p_{ij}, r_{ij})\n\\]\nThen we’ll summarize each run by a single management-style indicator:\n\n% of the landscape with suitability ≥ 0.7\n\nFinally, we’ll apply the Morris elementary-effects method to the weights \\(((w_\\text{slope}, w_\\text{perm}, w_\\text{rain}, w_\\text{int}))\\) to estimate μ★ and σ — showing which weights matter most and which behave nonlinearly or interactively.\n\n\nStep 1. Generate synthetic rasters with spatial structure\nWhat we will do:\nslope: increases toward one corner (like uplands vs basin floor).\nperm: hotspot of high permeability (like an alluvial fan or paleo-channel).\nrain: west-to-east gradient plus a sinusoidal north-south climate band.\n\n\nnp.random.seed(42)\n\n# grid size\nnx, ny = 50, 50\n\n# create coordinate grids (to allow gradients across space)\nx = np.linspace(0, 1, nx)\ny = np.linspace(0, 1, ny)\nX, Y = np.meshgrid(x, y, indexing=\"ij\")  # X[i,j], Y[i,j] in [0,1]\n\n# synthetic \"slope\": higher slope in the NE corner + noise\nslope = 30 * (0.3*X + 0.7*Y) + np.random.normal(0, 2, (nx, ny))\nslope = np.clip(slope, 0, 30)  # degrees\n\n# synthetic \"soil permeability\" (perm): better in valley-like band\nperm = 0.6 + 0.4*np.exp(-((X-0.5)**2 + (Y-0.2)**2)/0.05)\nperm += np.random.normal(0, 0.05, (nx, ny))\nperm = np.clip(perm, 0, 1)\n\n# synthetic \"rainfall\": gradient + hump\nrain = 200 + 600*(1 - X) + 80*np.sin(3*np.pi*Y)\nrain += np.random.normal(0, 20, (nx, ny))\nrain = np.clip(rain, 200, 800)  # mm/yr\n\nslope.shape, perm.shape, rain.shape\n\n((50, 50), (50, 50), (50, 50))\n\n\n\n\nStep 2. define the transformed suitability sub-scores\nNow we define the \\(f_{slope}, f_{perm}, f_{rain}\\), and the interaction term.\nDesign choices:\n\nWe want nonlinearity: e.g. very steep slopes get penalized sharply.\nWe want “Goldilocks” rainfall: mid-range rain gives best recharge; too little = no water, too much = maybe runoff.\nWe want interaction between perm and rain: rain only helps if perm is high.\n\n\ndef score_slope(slope_grid):\n    # high score for gentle slopes, decays nonlinearly\n    # slope in degrees 0..30\n    # we'll do an exponential decay so it's strongly nonlinear\n    return np.exp(-slope_grid / 10.0)  # near 1 at 0 deg, ~exp(-3)=0.05 at 30 deg\n\ndef score_perm(perm_grid):\n    # more permeable is better, roughly linear\n    # perm already in [0,1]\n    return perm_grid  # identity for now\n\ndef score_rain(rain_grid):\n    # peak around moderate rainfall (e.g. 500 mm),\n    # penalize too dry or too wet via a Gaussian-like curve\n    return np.exp(-((rain_grid - 500.0)**2) / (2*(100.0**2)))\n    # ~1.0 at 500 mm, drops off as you move away\n\ndef score_interaction(perm_grid, rain_grid):\n    # interaction: rain only \"counts\" if perm is high\n    # e.g. multiply them, so high perm + decent rain is very good\n    # but high rain with low perm won't help\n    rain_norm = (rain_grid - 200.0) / (800.0 - 200.0)  # scale rain to [0,1]\n    rain_norm = np.clip(rain_norm, 0, 1)\n    return perm_grid * rain_norm\n\n\n\nStep 3. Define the suitability model with weights\nwe will treat the weights as the “inputs” then we will perturb them with Morris, these are like decision-maker priorities in a WLC\n\ndef suitability_from_weights(w_slope, w_perm, w_rain, w_int):\n    \"\"\"\n    Given a set of weights, compute a final suitability raster,\n    then return a management-style scalar metric:\n    % of the landscape above a suitability threshold.\n    \"\"\"\n    S_slope = score_slope(slope)\n    S_perm  = score_perm(perm)\n    S_rain  = score_rain(rain)\n    S_int   = score_interaction(perm, rain)\n\n    # weighted linear combo including interaction layer\n    suitability = (\n        w_slope * S_slope +\n        w_perm  * S_perm  +\n        w_rain  * S_rain  +\n        w_int   * S_int\n    )\n\n    # normalize by sum of weights so scores stay in a comparable range\n    w_sum = (w_slope + w_perm + w_rain + w_int) + 1e-12\n    suitability = suitability / w_sum\n\n    # management-style scalar output:\n    # fraction of cells with suitability &gt;= 0.7\n    high_priority = (suitability &gt;= 0.7).mean()  # this is a single number between 0 and 1\n\n    return float(high_priority)\n\n\n\nStep 4. Run Morris-style elementary effects on the weights\nThis is almost identical to what we did with penguins, but now:\n\nWe’re sampling weights instead of morphology.\nWe’re perturbing one weight at a time.\nWe’ll say each weight varies in [0.1, 1.0] — like minimum importance to strong importance — and we’ll use a fractional step.\n\n\nweight_ranges = {\n    \"w_slope\": (0.1, 1.0),\n    \"w_perm\":  (0.1, 1.0),\n    \"w_rain\":  (0.1, 1.0),\n    \"w_int\":   (0.1, 1.0),\n}\n\ndef sample_elementary_effects_wlc(model_func, weight_ranges, n_samples=100, delta=0.1):\n    \"\"\"\n    model_func: suitability_from_weights\n    weight_ranges: dict of {weight_name: (min,max)}\n    n_samples: number of random base weight sets to test\n    delta: step as a fraction of that weight's range\n    \"\"\"\n    wnames = list(weight_ranges.keys())\n    records = []\n\n    for _ in range(n_samples):\n        # pick a random baseline weight set, not at the extreme edges\n        base_w = {}\n        for wname, (lo, hi) in weight_ranges.items():\n            base_w[wname] = np.random.uniform(\n                lo + 0.1*(hi-lo),\n                hi - 0.1*(hi-lo)\n            )\n\n        # baseline model output\n        y0 = model_func(\n            base_w[\"w_slope\"],\n            base_w[\"w_perm\"],\n            base_w[\"w_rain\"],\n            base_w[\"w_int\"]\n        )\n\n        ee_point = {}\n        # perturb each weight in turn\n        for wname, (lo, hi) in weight_ranges.items():\n            step = delta * (hi - lo)\n\n            perturbed_w = base_w.copy()\n            perturbed_w[wname] = np.clip(base_w[wname] + step, lo, hi)\n\n            y1 = model_func(\n                perturbed_w[\"w_slope\"],\n                perturbed_w[\"w_perm\"],\n                perturbed_w[\"w_rain\"],\n                perturbed_w[\"w_int\"]\n            )\n\n            # elementary effect for this weight\n            denom = step if step &gt; 1e-12 else 1e-12\n            ee_point[f\"{wname}_EE\"] = (y1 - y0) / denom\n\n        records.append(ee_point)\n\n    return pd.DataFrame(records)\n\nee_wlc_df = sample_elementary_effects_wlc(\n    suitability_from_weights,\n    weight_ranges,\n    n_samples=100,\n    delta=0.1\n)\n\nee_wlc_df.head()\n\n\n\n\n\n\n\n\nw_slope_EE\nw_perm_EE\nw_rain_EE\nw_int_EE\n\n\n\n\n0\n0.008889\n0.013333\n0.013333\n-0.017778\n\n\n1\n-0.106667\n0.066667\n0.044444\n-0.053333\n\n\n2\n-0.137778\n0.075556\n0.080000\n-0.111111\n\n\n3\n0.000000\n0.044444\n0.008889\n-0.008889\n\n\n4\n-0.004444\n0.026667\n0.031111\n-0.022222\n\n\n\n\n\n\n\n\nNow we have:\n\nEach row = one baseline weighting scenario.\nEach column = how sensitive the “% high-priority land” is to each weight at that baseline.\nThis is a spatial decision model, with real interactions baked in (w_int affects that interaction layer we defined).\n\n\n\n\nStep 5. Summarize \\(\\mu^*\\) and \\(\\sigma\\) for the weights\n\nsummary_wlc = pd.DataFrame({\n    \"mu_star\": [ee_wlc_df[c].abs().mean() for c in ee_wlc_df.columns],\n    \"sigma\":   [ee_wlc_df[c].std(ddof=1)  for c in ee_wlc_df.columns],\n}, index=[c.replace(\"_EE\", \"\") for c in ee_wlc_df.columns])\n\nsummary_wlc\n\n\n\n\n\n\n\n\nmu_star\nsigma\n\n\n\n\nw_slope\n0.059733\n0.048370\n\n\nw_perm\n0.053911\n0.026679\n\n\nw_rain\n0.047244\n0.035660\n\n\nw_int\n0.053956\n0.034369\n\n\n\n\n\n\n\n\nConceptual Questions\n\nWhich weight most strongly controls (\\(\\mu^*\\)) how much of the map is deemed suitable or good (gte 7) Which weight’s effect is stable vs. only matters in some regimes? Does the interaction weight (w_int) have a high \\(\\sigma\\)? (it should because we designed it to matter more when both permeability and rain are favorable)\n\n\n\n\nStep 6. plot \\(\\mu^*\\) vs \\(\\sigma\\) for the spatial model\n\nmu_vals = summary_wlc[\"mu_star\"].values.astype(float)\nsigma_vals = summary_wlc[\"sigma\"].values.astype(float)\nnames = summary_wlc.index.tolist()\n\nmu_mean = float(np.mean(mu_vals))\nsigma_mean = float(np.mean(sigma_vals))\n\n# define plotting limits with a small % padding around actual data\nmu_min = mu_vals.min()\nmu_max = mu_vals.max()\nsigma_min = sigma_vals.min()\nsigma_max = sigma_vals.max()\n\nmu_pad = 0.1 * (mu_max - mu_min if mu_max &gt; mu_min else 1.0)\nsigma_pad = 0.1 * (sigma_max - sigma_min if sigma_max &gt; sigma_min else 1.0)\n\nx_lo = mu_min - mu_pad\nx_hi = mu_max + mu_pad\ny_lo = sigma_min - sigma_pad\ny_hi = sigma_max + sigma_pad\n\nfig, ax = plt.subplots(figsize=(6,5))\n\n# scatter points\nax.scatter(mu_vals, sigma_vals,\n           s=100, color=\"teal\", edgecolor=\"k\", zorder=3)\n\n# adaptive label offset: 2% of axis span instead of hardcoded 0.02\nx_offset = 0.02 * (x_hi - x_lo)\ny_offset = 0.02 * (y_hi - y_lo)\n\nfor x, y, label in zip(mu_vals, sigma_vals, names):\n    ax.text(x + x_offset, y + y_offset, label,\n            fontsize=10, zorder=4)\n\n# reference lines\nax.axvline(mu_mean, color=\"gray\", linestyle=\"--\", alpha=0.6,\n           label=f\"mean μ* = {mu_mean:.2f}\")\nax.axhline(sigma_mean, color=\"gray\", linestyle=\"--\", alpha=0.6,\n           label=f\"mean σ = {sigma_mean:.2e}\")\n\n# set sane limits\nax.set_xlim(x_lo, x_hi)\nax.set_ylim(y_lo, y_hi)\n\nax.set_xlabel(\"μ* (overall influence on predicted mass)\")\nax.set_ylabel(\"σ (interaction / nonlinearity)\")\nax.set_title(\"Morris Sensitivity Screening — Penguin Body Mass Model\")\nax.grid(alpha=0.3)\nax.legend(loc=\"upper left\", frameon=False)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nInterpretation\nThe plot above shows the Morris μ★–σ diagram for our synthetic spatial suitability model. Each point represents one of the four input weights varied in the analysis:\n\n\\(\\mu^*\\) (x-axis) — measures the overall influence of that weight on the model output (mean absolute elementary effect). A larger \\(\\mu^*\\) means that changing this weight tends to change the suitability outcome more strongly — i.e., it is a more influential parameter overall.\n\\(\\sigma\\) (y-axis) — measures the variability of those effects across the parameter space. A higher \\(\\sigma\\) indicates nonlinear behavior or interactions with other parameters. In other words, the effect of that weight depends on the values of other weights or on specific regions of the input space.\n\n\nWhat the plot shows:\n\\(w_{slope}\\) shows the highest \\(\\mu^*\\) — meaning that the slope criterion is the dominant control on how much land is classified as suitable. This makes intuitive sense: slope strongly affects recharge potential, and our transformation for slope was nonlinear, creating steep contrasts between gentle and steep terrain.\n\\(w_{perm}\\) and \\(w_{rain}\\) have moderate \\(\\mu^*\\) values — they influence suitability, but to a smaller degree. Their relatively low \\(\\sigma\\) values suggest that their effects are more linear and consistent across different weight combinations.\n\\(w_{int}\\) (the interaction weight) has a slightly lower \\(\\mu^*\\) than \\(w_{slope}\\) but a comparable or higher \\(\\sigma\\) — meaning it contributes less to the total suitability on average but behaves nonlinearly. This reflects the fact that its impact depends on the combination of permeability and rainfall, not on either one alone.\ntherefore: - The slope weight is the most critical lever — changing it will have the greatest and most consistent effect on the area classified as “high-suitability.”\n\nThe interaction term is important to monitor — it introduces context-dependent variability and captures coupled effects of rainfall and permeability.\nThe other weights behave more linearly, meaning that their influence can be anticipated more easily and modeled with less uncertainty.\n\nThis is a hallmark Morris result:\n\nParameters with high \\(\\mu^*\\) and low \\(\\sigma\\) → strong, predictable influence.\nParameters with moderate \\(\\mu^*\\) but high \\(\\sigma\\) → interactive or nonlinear influence.\nParameters with low \\(\\mu^*\\) and low \\(\\sigma\\) → negligible or near-linear effect.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "Notebooks/Morris_Elementary_Effects.html#review-questions",
    "href": "Notebooks/Morris_Elementary_Effects.html#review-questions",
    "title": "Morris Sensitivity Analysis (Elementary Effects Method)",
    "section": "Review Questions",
    "text": "Review Questions\n\nConceptual\n\nWhat is the main purpose of the Morris elementary effects method in sensitivity analysis?\nHow does Morris differ from a simple one-at-a-time (OAT) sensitivity analysis?\nIn the Morris method, what does the elementary effect represent for a given input parameter?\nWhy do we calculate both \\(\\mu^*\\) (mu-star) and \\(\\sigma\\) (sigma), and what does each measure tell us?\nWhat does a high \\(\\mu^*\\) combined with a low \\(\\sigma\\) imply about a model parameter?\nWhat does a high \\(\\sigma\\) (standard deviation of elementary effects) indicate about a parameter’s behavior?\nHow do nonlinearities or interactions among variables influence \\(\\sigma\\) in Morris results?\nWhy is Morris sometimes called a “screening” method rather than a “full” global sensitivity method?\nWhat is the role of \\(\\Delta\\) (delta) in the Morris method, and how is it typically chosen?\nIn a spatial MCDA context, what might a model input represent, and what would the model output represent in a Morris analysis?\n\n\n\nInterpretation\n\nOn a \\(\\mu^*\\)–\\(\\sigma\\) plot, where would you expect to find a highly linear and dominant variable?\nWhat does it mean if a point is high on the σ axis but low on μ★?\nIn the penguin example, why did σ values collapse toward zero?\nIn the synthetic raster model, why does the interaction term (w_int) often have a high \\(\\sigma\\)?\nHow might a decision-maker interpret high \\(\\sigma\\) in a spatial suitability model?\n\n\n\nAnswers\n\n1. The main purpose of the Morris elementary effects method is to identify which input variables have the greatest overall influence on a model’s output, and to detect whether their effects are linear, nonlinear, or involve interactions with other inputs.\n\n\n2. Unlike a simple one-at-a-time (OAT) analysis that tests variables around a single baseline, Morris repeats OAT experiments across many random starting points in the input space, allowing it to capture global (not just local) sensitivity patterns.\n\n\n3. The elementary effect for a given parameter represents the change in model output resulting from a small perturbation of that parameter, divided by the size of the change (\\(\\Delta\\)). It approximates the local slope of the model’s response surface.\n\n\n4. \\(\\mu^*\\) measures the average magnitude of influence of an input (its overall importance), while σ measures how variable that influence is across the input space (nonlinearity or interaction effects).\n\n\n5. A high \\(\\mu^*\\) and low \\(\\sigma\\) indicate a parameter that has a strong, consistent, and largely linear effect on the model output.\n\n\n6. A high \\(\\sigma\\) means the parameter’s influence changes across the input space — suggesting nonlinear behavior or interactions with other parameters.\n\n\n7. Nonlinearities or interactions cause \\(\\sigma\\) to increase because the sign or magnitude of the elementary effects vary depending on where in the parameter space the sample is taken.\n\n\n8. Morris is called a “screening” method because it efficiently identifies which parameters are important without requiring full global variance decomposition (like Sobol or FAST). It’s often used as a first step before more detailed analyses.\n\n\n9. \\(\\Delta\\) (delta) defines the size of the perturbation for each elementary effect. It is typically a small fraction (e.g., 0.1–0.2) of the parameter’s total range to ensure that local changes approximate the model’s gradient while still exploring meaningful variability.\n\n\n10. In a spatial MCDA, model inputs might represent layer weights (e.g., slope, permeability, rainfall importance), while the output could represent an aggregated measure such as overall suitability, recharge potential, or % of land above a suitability threshold.\n\n\n\n11. A highly linear and dominant variable appears toward the lower right of the \\(\\mu^*\\)–\\(\\sigma\\) plot — high \\(\\mu^*\\)(strong influence) and low \\(\\sigma\\) (consistent, linear behavior).\n\n\n12. A point high on the \\(\\sigma\\) axis but low on \\(\\mu^*\\) represents a parameter with weak average influence but strong nonlinear or context-dependent effects.\n\n\n13. In the penguin example, \\(\\sigma\\) values collapsed toward zero because the regression model was purely linear and additive, so each variable’s effect was constant and did not vary with the others.\n\n\n14. In the synthetic raster model, the interaction weight (w_int) often has a high σ because its influence depends jointly on permeability and rainfall — the effect is strong only where both variables are favorable.\n\n\n15. A decision-maker might interpret high \\(\\sigma\\) as an indicator of uncertainty or context dependency — meaning that the importance of that criterion changes across the landscape or depends on how other factors are weighted.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Morris Sensitivity Analysis (Elementary Effects Method)</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]